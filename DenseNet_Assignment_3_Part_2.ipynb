{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JAZ3rvN3LJ0v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import Add,concatenate\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "nGoZY5vBRLa8",
    "outputId": "02fd21e5-06b6-40b3-a5d1-bd4a655337ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T_N74Mf4RLw7",
    "outputId": "c7740d09-3cc3-4442-ed5d-4c71e85cd66d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/ECE 657/Part-2\n"
     ]
    }
   ],
   "source": [
    "cd 'drive/My Drive/ECE 657/Part-2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "PxrsBUjORU2v",
    "outputId": "63cb7bf8-5fdd-437c-9f70-26f1a2ae1d9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>...</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "      <th>769</th>\n",
       "      <th>770</th>\n",
       "      <th>771</th>\n",
       "      <th>772</th>\n",
       "      <th>773</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.00000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.00000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.00000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.00000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39999.500000</td>\n",
       "      <td>2.002933</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.006067</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.090450</td>\n",
       "      <td>0.241383</td>\n",
       "      <td>0.392517</td>\n",
       "      <td>0.777333</td>\n",
       "      <td>2.158300</td>\n",
       "      <td>5.597000</td>\n",
       "      <td>14.485433</td>\n",
       "      <td>32.866333</td>\n",
       "      <td>45.783900</td>\n",
       "      <td>51.985883</td>\n",
       "      <td>50.779350</td>\n",
       "      <td>50.425750</td>\n",
       "      <td>53.686350</td>\n",
       "      <td>51.371150</td>\n",
       "      <td>40.467633</td>\n",
       "      <td>25.962700</td>\n",
       "      <td>10.281083</td>\n",
       "      <td>4.034833</td>\n",
       "      <td>1.929250</td>\n",
       "      <td>1.148100</td>\n",
       "      <td>0.842217</td>\n",
       "      <td>0.47675</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.08960</td>\n",
       "      <td>0.015167</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>0.102850</td>\n",
       "      <td>0.339433</td>\n",
       "      <td>0.741917</td>\n",
       "      <td>2.058883</td>\n",
       "      <td>7.226483</td>\n",
       "      <td>17.394200</td>\n",
       "      <td>32.014417</td>\n",
       "      <td>53.019567</td>\n",
       "      <td>...</td>\n",
       "      <td>92.459967</td>\n",
       "      <td>90.458233</td>\n",
       "      <td>79.041600</td>\n",
       "      <td>62.473383</td>\n",
       "      <td>46.021650</td>\n",
       "      <td>44.367617</td>\n",
       "      <td>47.964333</td>\n",
       "      <td>38.793683</td>\n",
       "      <td>22.335200</td>\n",
       "      <td>9.189950</td>\n",
       "      <td>3.814733</td>\n",
       "      <td>0.622083</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>0.220950</td>\n",
       "      <td>1.03375</td>\n",
       "      <td>4.379250</td>\n",
       "      <td>12.673967</td>\n",
       "      <td>21.02115</td>\n",
       "      <td>21.219867</td>\n",
       "      <td>17.159950</td>\n",
       "      <td>22.219717</td>\n",
       "      <td>30.688333</td>\n",
       "      <td>41.950650</td>\n",
       "      <td>51.574867</td>\n",
       "      <td>52.491717</td>\n",
       "      <td>46.289000</td>\n",
       "      <td>42.470617</td>\n",
       "      <td>45.183467</td>\n",
       "      <td>50.020433</td>\n",
       "      <td>46.359133</td>\n",
       "      <td>34.545900</td>\n",
       "      <td>23.297883</td>\n",
       "      <td>16.652150</td>\n",
       "      <td>17.953617</td>\n",
       "      <td>22.969333</td>\n",
       "      <td>17.967350</td>\n",
       "      <td>8.525333</td>\n",
       "      <td>2.753883</td>\n",
       "      <td>0.832950</td>\n",
       "      <td>0.072850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17320.652413</td>\n",
       "      <td>1.415000</td>\n",
       "      <td>0.061507</td>\n",
       "      <td>0.292286</td>\n",
       "      <td>1.178115</td>\n",
       "      <td>2.306445</td>\n",
       "      <td>4.365543</td>\n",
       "      <td>5.702057</td>\n",
       "      <td>8.023953</td>\n",
       "      <td>13.942884</td>\n",
       "      <td>23.572943</td>\n",
       "      <td>38.252977</td>\n",
       "      <td>57.692077</td>\n",
       "      <td>65.947637</td>\n",
       "      <td>68.565341</td>\n",
       "      <td>67.919718</td>\n",
       "      <td>67.397257</td>\n",
       "      <td>69.421676</td>\n",
       "      <td>67.915419</td>\n",
       "      <td>62.725940</td>\n",
       "      <td>51.620728</td>\n",
       "      <td>32.300648</td>\n",
       "      <td>20.015361</td>\n",
       "      <td>13.871987</td>\n",
       "      <td>10.593331</td>\n",
       "      <td>9.214058</td>\n",
       "      <td>7.01254</td>\n",
       "      <td>5.295104</td>\n",
       "      <td>2.78752</td>\n",
       "      <td>1.048501</td>\n",
       "      <td>0.361104</td>\n",
       "      <td>1.133843</td>\n",
       "      <td>3.104964</td>\n",
       "      <td>6.149413</td>\n",
       "      <td>9.589874</td>\n",
       "      <td>15.716790</td>\n",
       "      <td>29.936239</td>\n",
       "      <td>47.877737</td>\n",
       "      <td>65.585449</td>\n",
       "      <td>82.247678</td>\n",
       "      <td>...</td>\n",
       "      <td>100.881732</td>\n",
       "      <td>99.700169</td>\n",
       "      <td>95.036194</td>\n",
       "      <td>88.766993</td>\n",
       "      <td>78.686045</td>\n",
       "      <td>77.236284</td>\n",
       "      <td>83.267721</td>\n",
       "      <td>74.407819</td>\n",
       "      <td>55.982185</td>\n",
       "      <td>37.736031</td>\n",
       "      <td>24.425300</td>\n",
       "      <td>8.175978</td>\n",
       "      <td>1.009238</td>\n",
       "      <td>4.416291</td>\n",
       "      <td>10.07838</td>\n",
       "      <td>20.894199</td>\n",
       "      <td>37.020302</td>\n",
       "      <td>49.07243</td>\n",
       "      <td>48.312498</td>\n",
       "      <td>42.208942</td>\n",
       "      <td>48.438714</td>\n",
       "      <td>55.116266</td>\n",
       "      <td>62.242895</td>\n",
       "      <td>68.555227</td>\n",
       "      <td>69.759801</td>\n",
       "      <td>67.367902</td>\n",
       "      <td>66.182219</td>\n",
       "      <td>66.754477</td>\n",
       "      <td>68.685723</td>\n",
       "      <td>66.060666</td>\n",
       "      <td>57.569109</td>\n",
       "      <td>48.952580</td>\n",
       "      <td>42.147395</td>\n",
       "      <td>43.985951</td>\n",
       "      <td>51.856475</td>\n",
       "      <td>45.225807</td>\n",
       "      <td>29.529071</td>\n",
       "      <td>17.396211</td>\n",
       "      <td>9.263115</td>\n",
       "      <td>2.131444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24999.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39999.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>54999.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>69999.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>239.00000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>185.00000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>230.00000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>255.00000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>170.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id         Label  ...           783           784\n",
       "count  60000.000000  60000.000000  ...  60000.000000  60000.000000\n",
       "mean   39999.500000      2.002933  ...      0.832950      0.072850\n",
       "std    17320.652413      1.415000  ...      9.263115      2.131444\n",
       "min    10000.000000      0.000000  ...      0.000000      0.000000\n",
       "25%    24999.750000      1.000000  ...      0.000000      0.000000\n",
       "50%    39999.500000      2.000000  ...      0.000000      0.000000\n",
       "75%    54999.250000      3.000000  ...      0.000000      0.000000\n",
       "max    69999.000000      4.000000  ...    255.000000    170.000000\n",
       "\n",
       "[8 rows x 786 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('testX.csv')\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OXMYYGO8RvbV",
    "outputId": "8844d045-afdf-4a1f-c593-41ef0285e586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ASARiWKdR1eS",
    "outputId": "5eed690b-e888-4adb-b3c5-77ab9bf25a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 28, 28, 1)\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_classes = 5\n",
    "epochs = 300\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "x_train = train.drop(['Label', 'Id'], axis=1).to_numpy()\n",
    "y_train = train['Label'].to_numpy()\n",
    "x_test = test.drop(['Id'], axis = 1).to_numpy()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = 42)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tensorflow.keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_val /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NE-BSa9pSJem"
   },
   "outputs": [],
   "source": [
    "#DenseNet\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0008), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wb811_qWUtHT",
    "outputId": "48f0fe19-6583-44df-bead-1769beb37330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4895 - accuracy: 0.7924 - val_loss: 0.4536 - val_accuracy: 0.8073\n",
      "Epoch 2/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4665 - accuracy: 0.8041 - val_loss: 0.4365 - val_accuracy: 0.8283\n",
      "Epoch 3/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4415 - accuracy: 0.8165 - val_loss: 0.4197 - val_accuracy: 0.8288\n",
      "Epoch 4/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4187 - accuracy: 0.8255 - val_loss: 0.4117 - val_accuracy: 0.8307\n",
      "Epoch 5/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4082 - accuracy: 0.8297 - val_loss: 0.4367 - val_accuracy: 0.8217\n",
      "Epoch 6/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3911 - accuracy: 0.8386 - val_loss: 0.4884 - val_accuracy: 0.7960\n",
      "Epoch 7/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3780 - accuracy: 0.8445 - val_loss: 0.4021 - val_accuracy: 0.8390\n",
      "Epoch 8/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3779 - accuracy: 0.8431 - val_loss: 0.4037 - val_accuracy: 0.8377\n",
      "Epoch 9/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3535 - accuracy: 0.8544 - val_loss: 0.4113 - val_accuracy: 0.8320\n",
      "Epoch 10/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3465 - accuracy: 0.8570 - val_loss: 0.3981 - val_accuracy: 0.8410\n",
      "Epoch 11/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3376 - accuracy: 0.8614 - val_loss: 0.3941 - val_accuracy: 0.8375\n",
      "Epoch 12/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3344 - accuracy: 0.8621 - val_loss: 0.3823 - val_accuracy: 0.8500\n",
      "Epoch 13/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3233 - accuracy: 0.8672 - val_loss: 0.3803 - val_accuracy: 0.8462\n",
      "Epoch 14/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3115 - accuracy: 0.8722 - val_loss: 0.3655 - val_accuracy: 0.8542\n",
      "Epoch 15/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3155 - accuracy: 0.8706 - val_loss: 0.3722 - val_accuracy: 0.8542\n",
      "Epoch 16/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2922 - accuracy: 0.8813 - val_loss: 0.4028 - val_accuracy: 0.8415\n",
      "Epoch 17/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2928 - accuracy: 0.8798 - val_loss: 0.3670 - val_accuracy: 0.8570\n",
      "Epoch 18/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2843 - accuracy: 0.8853 - val_loss: 0.4419 - val_accuracy: 0.8223\n",
      "Epoch 19/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2846 - accuracy: 0.8827 - val_loss: 0.3771 - val_accuracy: 0.8607\n",
      "Epoch 20/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2916 - accuracy: 0.8807 - val_loss: 0.4529 - val_accuracy: 0.8128\n",
      "Epoch 21/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2866 - accuracy: 0.8814 - val_loss: 0.3749 - val_accuracy: 0.8583\n",
      "Epoch 22/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2636 - accuracy: 0.8932 - val_loss: 0.3779 - val_accuracy: 0.8530\n",
      "Epoch 23/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2558 - accuracy: 0.8975 - val_loss: 0.3553 - val_accuracy: 0.8642\n",
      "Epoch 24/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2517 - accuracy: 0.8981 - val_loss: 0.3712 - val_accuracy: 0.8628\n",
      "Epoch 25/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2592 - accuracy: 0.8958 - val_loss: 0.3623 - val_accuracy: 0.8652\n",
      "Epoch 26/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2438 - accuracy: 0.9012 - val_loss: 0.3913 - val_accuracy: 0.8565\n",
      "Epoch 27/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2462 - accuracy: 0.9014 - val_loss: 0.3691 - val_accuracy: 0.8615\n",
      "Epoch 28/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2459 - accuracy: 0.9018 - val_loss: 0.3731 - val_accuracy: 0.8623\n",
      "Epoch 29/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2404 - accuracy: 0.9033 - val_loss: 0.3781 - val_accuracy: 0.8630\n",
      "Epoch 30/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2402 - accuracy: 0.9015 - val_loss: 0.4284 - val_accuracy: 0.8360\n",
      "Epoch 31/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2358 - accuracy: 0.9047 - val_loss: 0.3767 - val_accuracy: 0.8625\n",
      "Epoch 32/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2304 - accuracy: 0.9069 - val_loss: 0.3779 - val_accuracy: 0.8630\n",
      "Epoch 33/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2309 - accuracy: 0.9062 - val_loss: 0.3827 - val_accuracy: 0.8555\n",
      "Epoch 34/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2332 - accuracy: 0.9063 - val_loss: 0.3751 - val_accuracy: 0.8672\n",
      "Epoch 35/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2208 - accuracy: 0.9111 - val_loss: 0.3798 - val_accuracy: 0.8617\n",
      "Epoch 36/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2213 - accuracy: 0.9117 - val_loss: 0.3875 - val_accuracy: 0.8597\n",
      "Epoch 37/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2147 - accuracy: 0.9136 - val_loss: 0.3943 - val_accuracy: 0.8553\n",
      "Epoch 38/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2187 - accuracy: 0.9103 - val_loss: 0.4167 - val_accuracy: 0.8533\n",
      "Epoch 39/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2095 - accuracy: 0.9164 - val_loss: 0.3650 - val_accuracy: 0.8695\n",
      "Epoch 40/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2058 - accuracy: 0.9172 - val_loss: 0.3971 - val_accuracy: 0.8550\n",
      "Epoch 41/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2196 - accuracy: 0.9102 - val_loss: 0.3813 - val_accuracy: 0.8663\n",
      "Epoch 42/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2010 - accuracy: 0.9189 - val_loss: 0.3691 - val_accuracy: 0.8717\n",
      "Epoch 43/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1940 - accuracy: 0.9219 - val_loss: 0.3823 - val_accuracy: 0.8688\n",
      "Epoch 44/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2033 - accuracy: 0.9180 - val_loss: 0.3849 - val_accuracy: 0.8648\n",
      "Epoch 45/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1917 - accuracy: 0.9236 - val_loss: 0.3784 - val_accuracy: 0.8683\n",
      "Epoch 46/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1939 - accuracy: 0.9222 - val_loss: 0.4095 - val_accuracy: 0.8553\n",
      "Epoch 47/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1947 - accuracy: 0.9208 - val_loss: 0.3865 - val_accuracy: 0.8717\n",
      "Epoch 48/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1853 - accuracy: 0.9263 - val_loss: 0.4075 - val_accuracy: 0.8618\n",
      "Epoch 49/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1964 - accuracy: 0.9209 - val_loss: 0.3924 - val_accuracy: 0.8692\n",
      "Epoch 50/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1913 - accuracy: 0.9238 - val_loss: 0.4045 - val_accuracy: 0.8677\n",
      "Epoch 51/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1932 - accuracy: 0.9225 - val_loss: 0.3733 - val_accuracy: 0.8732\n",
      "Epoch 52/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1795 - accuracy: 0.9274 - val_loss: 0.3913 - val_accuracy: 0.8727\n",
      "Epoch 53/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1836 - accuracy: 0.9267 - val_loss: 0.3797 - val_accuracy: 0.8728\n",
      "Epoch 54/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1776 - accuracy: 0.9294 - val_loss: 0.3766 - val_accuracy: 0.8735\n",
      "Epoch 55/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1779 - accuracy: 0.9300 - val_loss: 0.4023 - val_accuracy: 0.8692\n",
      "Epoch 56/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1818 - accuracy: 0.9269 - val_loss: 0.3930 - val_accuracy: 0.8715\n",
      "Epoch 57/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1745 - accuracy: 0.9303 - val_loss: 0.4259 - val_accuracy: 0.8607\n",
      "Epoch 58/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.9297 - val_loss: 0.3920 - val_accuracy: 0.8720\n",
      "Epoch 59/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1691 - accuracy: 0.9337 - val_loss: 0.4028 - val_accuracy: 0.8697\n",
      "Epoch 60/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1741 - accuracy: 0.9312 - val_loss: 0.3967 - val_accuracy: 0.8707\n",
      "Epoch 61/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1730 - accuracy: 0.9315 - val_loss: 0.4030 - val_accuracy: 0.8688\n",
      "Epoch 62/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1724 - accuracy: 0.9314 - val_loss: 0.3979 - val_accuracy: 0.8697\n",
      "Epoch 63/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1662 - accuracy: 0.9347 - val_loss: 0.4131 - val_accuracy: 0.8607\n",
      "Epoch 64/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1834 - accuracy: 0.9260 - val_loss: 0.4131 - val_accuracy: 0.8618\n",
      "Epoch 65/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1655 - accuracy: 0.9344 - val_loss: 0.3874 - val_accuracy: 0.8737\n",
      "Epoch 66/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1728 - accuracy: 0.9312 - val_loss: 0.4580 - val_accuracy: 0.8465\n",
      "Epoch 67/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1641 - accuracy: 0.9351 - val_loss: 0.4057 - val_accuracy: 0.8763\n",
      "Epoch 68/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1734 - accuracy: 0.9299 - val_loss: 0.3974 - val_accuracy: 0.8690\n",
      "Epoch 69/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1571 - accuracy: 0.9380 - val_loss: 0.4166 - val_accuracy: 0.8698\n",
      "Epoch 70/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1506 - accuracy: 0.9404 - val_loss: 0.4108 - val_accuracy: 0.8735\n",
      "Epoch 71/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.9375 - val_loss: 0.3955 - val_accuracy: 0.8777\n",
      "Epoch 72/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1681 - accuracy: 0.9322 - val_loss: 0.3908 - val_accuracy: 0.8767\n",
      "Epoch 73/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1477 - accuracy: 0.9424 - val_loss: 0.4093 - val_accuracy: 0.8763\n",
      "Epoch 74/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1547 - accuracy: 0.9386 - val_loss: 0.4242 - val_accuracy: 0.8682\n",
      "Epoch 75/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1537 - accuracy: 0.9379 - val_loss: 0.4710 - val_accuracy: 0.8557\n",
      "Epoch 76/300\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1579 - accuracy: 0.9375 - val_loss: 0.4742 - val_accuracy: 0.8508\n",
      "Epoch 77/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1537 - accuracy: 0.9402 - val_loss: 0.3935 - val_accuracy: 0.8745\n",
      "Epoch 78/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1533 - accuracy: 0.9392 - val_loss: 0.4408 - val_accuracy: 0.8632\n",
      "Epoch 79/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9364 - val_loss: 0.4028 - val_accuracy: 0.8733\n",
      "Epoch 80/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1544 - accuracy: 0.9387 - val_loss: 0.4144 - val_accuracy: 0.8782\n",
      "Epoch 81/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9453 - val_loss: 0.4358 - val_accuracy: 0.8763\n",
      "Epoch 82/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1505 - accuracy: 0.9405 - val_loss: 0.4192 - val_accuracy: 0.8747\n",
      "Epoch 83/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1411 - accuracy: 0.9446 - val_loss: 0.4231 - val_accuracy: 0.8730\n",
      "Epoch 84/300\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1423 - accuracy: 0.9424 - val_loss: 0.4535 - val_accuracy: 0.8645\n",
      "Epoch 85/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1394 - accuracy: 0.9455 - val_loss: 0.4049 - val_accuracy: 0.8800\n",
      "Epoch 86/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1507 - accuracy: 0.9400 - val_loss: 0.4444 - val_accuracy: 0.8665\n",
      "Epoch 87/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1436 - accuracy: 0.9435 - val_loss: 0.4317 - val_accuracy: 0.8743\n",
      "Epoch 88/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1389 - accuracy: 0.9460 - val_loss: 0.4143 - val_accuracy: 0.8807\n",
      "Epoch 89/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1426 - accuracy: 0.9420 - val_loss: 0.4331 - val_accuracy: 0.8640\n",
      "Epoch 90/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1441 - accuracy: 0.9423 - val_loss: 0.4221 - val_accuracy: 0.8768\n",
      "Epoch 91/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1401 - accuracy: 0.9443 - val_loss: 0.4184 - val_accuracy: 0.8765\n",
      "Epoch 92/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1384 - accuracy: 0.9456 - val_loss: 0.4259 - val_accuracy: 0.8722\n",
      "Epoch 93/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1364 - accuracy: 0.9458 - val_loss: 0.4231 - val_accuracy: 0.8760\n",
      "Epoch 94/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1364 - accuracy: 0.9458 - val_loss: 0.4159 - val_accuracy: 0.8765\n",
      "Epoch 95/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1412 - accuracy: 0.9439 - val_loss: 0.4331 - val_accuracy: 0.8708\n",
      "Epoch 96/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1388 - accuracy: 0.9448 - val_loss: 0.4401 - val_accuracy: 0.8685\n",
      "Epoch 97/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1547 - accuracy: 0.9385 - val_loss: 0.4433 - val_accuracy: 0.8675\n",
      "Epoch 98/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1244 - accuracy: 0.9509 - val_loss: 0.4540 - val_accuracy: 0.8713\n",
      "Epoch 99/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1347 - accuracy: 0.9464 - val_loss: 0.4623 - val_accuracy: 0.8683\n",
      "Epoch 100/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9473 - val_loss: 0.4615 - val_accuracy: 0.8743\n",
      "Epoch 101/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1294 - accuracy: 0.9498 - val_loss: 0.4475 - val_accuracy: 0.8730\n",
      "Epoch 102/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1328 - accuracy: 0.9476 - val_loss: 0.4480 - val_accuracy: 0.8752\n",
      "Epoch 103/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1385 - accuracy: 0.9448 - val_loss: 0.4211 - val_accuracy: 0.8755\n",
      "Epoch 104/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1255 - accuracy: 0.9506 - val_loss: 0.4903 - val_accuracy: 0.8670\n",
      "Epoch 105/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9483 - val_loss: 0.4467 - val_accuracy: 0.8748\n",
      "Epoch 106/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1265 - accuracy: 0.9496 - val_loss: 0.4331 - val_accuracy: 0.8787\n",
      "Epoch 107/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 0.9511 - val_loss: 0.4463 - val_accuracy: 0.8710\n",
      "Epoch 108/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1277 - accuracy: 0.9490 - val_loss: 0.4415 - val_accuracy: 0.8763\n",
      "Epoch 109/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1204 - accuracy: 0.9526 - val_loss: 0.4522 - val_accuracy: 0.8753\n",
      "Epoch 110/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1421 - accuracy: 0.9427 - val_loss: 0.4597 - val_accuracy: 0.8675\n",
      "Epoch 111/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1311 - accuracy: 0.9475 - val_loss: 0.4295 - val_accuracy: 0.8762\n",
      "Epoch 112/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1181 - accuracy: 0.9540 - val_loss: 0.4406 - val_accuracy: 0.8805\n",
      "Epoch 113/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9517 - val_loss: 0.4560 - val_accuracy: 0.8742\n",
      "Epoch 114/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1237 - accuracy: 0.9521 - val_loss: 0.4642 - val_accuracy: 0.8690\n",
      "Epoch 115/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1254 - accuracy: 0.9501 - val_loss: 0.4530 - val_accuracy: 0.8718\n",
      "Epoch 116/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1210 - accuracy: 0.9528 - val_loss: 0.4657 - val_accuracy: 0.8672\n",
      "Epoch 117/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1260 - accuracy: 0.9511 - val_loss: 0.4548 - val_accuracy: 0.8703\n",
      "Epoch 118/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1172 - accuracy: 0.9539 - val_loss: 0.4751 - val_accuracy: 0.8670\n",
      "Epoch 119/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1180 - accuracy: 0.9539 - val_loss: 0.4488 - val_accuracy: 0.8783\n",
      "Epoch 120/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1281 - accuracy: 0.9495 - val_loss: 0.4507 - val_accuracy: 0.8733\n",
      "Epoch 121/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1216 - accuracy: 0.9529 - val_loss: 0.4415 - val_accuracy: 0.8840\n",
      "Epoch 122/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1091 - accuracy: 0.9570 - val_loss: 0.4560 - val_accuracy: 0.8762\n",
      "Epoch 123/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1223 - accuracy: 0.9528 - val_loss: 0.4366 - val_accuracy: 0.8813\n",
      "Epoch 124/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9481 - val_loss: 0.4922 - val_accuracy: 0.8637\n",
      "Epoch 125/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1269 - accuracy: 0.9511 - val_loss: 0.4518 - val_accuracy: 0.8720\n",
      "Epoch 126/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1185 - accuracy: 0.9544 - val_loss: 0.4509 - val_accuracy: 0.8765\n",
      "Epoch 127/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1186 - accuracy: 0.9524 - val_loss: 0.4484 - val_accuracy: 0.8790\n",
      "Epoch 128/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1080 - accuracy: 0.9580 - val_loss: 0.4745 - val_accuracy: 0.8707\n",
      "Epoch 129/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1105 - accuracy: 0.9564 - val_loss: 0.4614 - val_accuracy: 0.8772\n",
      "Epoch 130/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1130 - accuracy: 0.9557 - val_loss: 0.4680 - val_accuracy: 0.8697\n",
      "Epoch 131/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1176 - accuracy: 0.9550 - val_loss: 0.4527 - val_accuracy: 0.8773\n",
      "Epoch 132/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1110 - accuracy: 0.9574 - val_loss: 0.4735 - val_accuracy: 0.8692\n",
      "Epoch 133/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1140 - accuracy: 0.9545 - val_loss: 0.4690 - val_accuracy: 0.8740\n",
      "Epoch 134/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1150 - accuracy: 0.9546 - val_loss: 0.4749 - val_accuracy: 0.8697\n",
      "Epoch 135/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1104 - accuracy: 0.9569 - val_loss: 0.5221 - val_accuracy: 0.8555\n",
      "Epoch 136/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1214 - accuracy: 0.9520 - val_loss: 0.4477 - val_accuracy: 0.8758\n",
      "Epoch 137/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1063 - accuracy: 0.9587 - val_loss: 0.4498 - val_accuracy: 0.8792\n",
      "Epoch 138/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1088 - accuracy: 0.9571 - val_loss: 0.5502 - val_accuracy: 0.8515\n",
      "Epoch 139/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1079 - accuracy: 0.9579 - val_loss: 0.5672 - val_accuracy: 0.8518\n",
      "Epoch 140/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1120 - accuracy: 0.9565 - val_loss: 0.4493 - val_accuracy: 0.8752\n",
      "Epoch 141/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1040 - accuracy: 0.9604 - val_loss: 0.4518 - val_accuracy: 0.8747\n",
      "Epoch 142/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1218 - accuracy: 0.9522 - val_loss: 0.4412 - val_accuracy: 0.8738\n",
      "Epoch 143/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1110 - accuracy: 0.9564 - val_loss: 0.4693 - val_accuracy: 0.8680\n",
      "Epoch 144/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1069 - accuracy: 0.9578 - val_loss: 0.4776 - val_accuracy: 0.8773\n",
      "Epoch 145/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9586 - val_loss: 0.4859 - val_accuracy: 0.8723\n",
      "Epoch 146/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1089 - accuracy: 0.9577 - val_loss: 0.4669 - val_accuracy: 0.8723\n",
      "Epoch 147/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1205 - accuracy: 0.9528 - val_loss: 0.4857 - val_accuracy: 0.8658\n",
      "Epoch 148/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1175 - accuracy: 0.9535 - val_loss: 0.4619 - val_accuracy: 0.8745\n",
      "Epoch 149/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1076 - accuracy: 0.9573 - val_loss: 0.4759 - val_accuracy: 0.8753\n",
      "Epoch 150/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1047 - accuracy: 0.9592 - val_loss: 0.5251 - val_accuracy: 0.8612\n",
      "Epoch 151/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1060 - accuracy: 0.9591 - val_loss: 0.4658 - val_accuracy: 0.8733\n",
      "Epoch 152/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1053 - accuracy: 0.9590 - val_loss: 0.4829 - val_accuracy: 0.8798\n",
      "Epoch 153/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1008 - accuracy: 0.9610 - val_loss: 0.4888 - val_accuracy: 0.8765\n",
      "Epoch 154/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0986 - accuracy: 0.9614 - val_loss: 0.5020 - val_accuracy: 0.8727\n",
      "Epoch 155/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1017 - accuracy: 0.9614 - val_loss: 0.4838 - val_accuracy: 0.8782\n",
      "Epoch 156/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0973 - accuracy: 0.9625 - val_loss: 0.5230 - val_accuracy: 0.8655\n",
      "Epoch 157/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1066 - accuracy: 0.9582 - val_loss: 0.4832 - val_accuracy: 0.8743\n",
      "Epoch 158/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0939 - accuracy: 0.9633 - val_loss: 0.4938 - val_accuracy: 0.8787\n",
      "Epoch 159/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0966 - accuracy: 0.9631 - val_loss: 0.5149 - val_accuracy: 0.8698\n",
      "Epoch 160/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0986 - accuracy: 0.9619 - val_loss: 0.4660 - val_accuracy: 0.8775\n",
      "Epoch 161/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1026 - accuracy: 0.9600 - val_loss: 0.4722 - val_accuracy: 0.8793\n",
      "Epoch 162/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0961 - accuracy: 0.9631 - val_loss: 0.4861 - val_accuracy: 0.8727\n",
      "Epoch 163/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1013 - accuracy: 0.9598 - val_loss: 0.4673 - val_accuracy: 0.8752\n",
      "Epoch 164/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1015 - accuracy: 0.9614 - val_loss: 0.5402 - val_accuracy: 0.8503\n",
      "Epoch 165/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1196 - accuracy: 0.9532 - val_loss: 0.4545 - val_accuracy: 0.8793\n",
      "Epoch 166/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0944 - accuracy: 0.9641 - val_loss: 0.4863 - val_accuracy: 0.8767\n",
      "Epoch 167/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1015 - accuracy: 0.9599 - val_loss: 0.4790 - val_accuracy: 0.8767\n",
      "Epoch 168/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0909 - accuracy: 0.9637 - val_loss: 0.5220 - val_accuracy: 0.8705\n",
      "Epoch 169/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0988 - accuracy: 0.9623 - val_loss: 0.4830 - val_accuracy: 0.8783\n",
      "Epoch 170/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0932 - accuracy: 0.9652 - val_loss: 0.4621 - val_accuracy: 0.8808\n",
      "Epoch 171/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 0.9560 - val_loss: 0.4637 - val_accuracy: 0.8792\n",
      "Epoch 172/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1021 - accuracy: 0.9605 - val_loss: 0.4807 - val_accuracy: 0.8768\n",
      "Epoch 173/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9592 - val_loss: 0.4678 - val_accuracy: 0.8753\n",
      "Epoch 174/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0958 - accuracy: 0.9614 - val_loss: 0.4894 - val_accuracy: 0.8740\n",
      "Epoch 175/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0973 - accuracy: 0.9620 - val_loss: 0.4816 - val_accuracy: 0.8752\n",
      "Epoch 176/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1024 - accuracy: 0.9600 - val_loss: 0.4599 - val_accuracy: 0.8750\n",
      "Epoch 177/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0955 - accuracy: 0.9629 - val_loss: 0.5437 - val_accuracy: 0.8682\n",
      "Epoch 178/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0997 - accuracy: 0.9618 - val_loss: 0.5120 - val_accuracy: 0.8703\n",
      "Epoch 179/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0923 - accuracy: 0.9642 - val_loss: 0.4812 - val_accuracy: 0.8785\n",
      "Epoch 180/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0885 - accuracy: 0.9662 - val_loss: 0.5115 - val_accuracy: 0.8758\n",
      "Epoch 181/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0969 - accuracy: 0.9631 - val_loss: 0.4992 - val_accuracy: 0.8703\n",
      "Epoch 182/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0970 - accuracy: 0.9613 - val_loss: 0.5070 - val_accuracy: 0.8718\n",
      "Epoch 183/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0982 - accuracy: 0.9623 - val_loss: 0.5005 - val_accuracy: 0.8705\n",
      "Epoch 184/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1107 - accuracy: 0.9572 - val_loss: 0.4868 - val_accuracy: 0.8747\n",
      "Epoch 185/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0883 - accuracy: 0.9662 - val_loss: 0.5119 - val_accuracy: 0.8755\n",
      "Epoch 186/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0882 - accuracy: 0.9657 - val_loss: 0.4914 - val_accuracy: 0.8790\n",
      "Epoch 187/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0909 - accuracy: 0.9647 - val_loss: 0.5087 - val_accuracy: 0.8718\n",
      "Epoch 188/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0960 - accuracy: 0.9625 - val_loss: 0.4983 - val_accuracy: 0.8807\n",
      "Epoch 189/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0942 - accuracy: 0.9633 - val_loss: 0.5014 - val_accuracy: 0.8788\n",
      "Epoch 190/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0909 - accuracy: 0.9654 - val_loss: 0.5223 - val_accuracy: 0.8705\n",
      "Epoch 191/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0898 - accuracy: 0.9648 - val_loss: 0.5167 - val_accuracy: 0.8722\n",
      "Epoch 192/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0899 - accuracy: 0.9648 - val_loss: 0.5047 - val_accuracy: 0.8762\n",
      "Epoch 193/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0861 - accuracy: 0.9667 - val_loss: 0.5107 - val_accuracy: 0.8770\n",
      "Epoch 194/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0941 - accuracy: 0.9644 - val_loss: 0.5018 - val_accuracy: 0.8777\n",
      "Epoch 195/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0884 - accuracy: 0.9652 - val_loss: 0.5387 - val_accuracy: 0.8682\n",
      "Epoch 196/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0932 - accuracy: 0.9627 - val_loss: 0.4988 - val_accuracy: 0.8733\n",
      "Epoch 197/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0907 - accuracy: 0.9651 - val_loss: 0.5309 - val_accuracy: 0.8752\n",
      "Epoch 198/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0907 - accuracy: 0.9647 - val_loss: 0.5035 - val_accuracy: 0.8813\n",
      "Epoch 199/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1038 - accuracy: 0.9605 - val_loss: 0.5202 - val_accuracy: 0.8730\n",
      "Epoch 200/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0865 - accuracy: 0.9672 - val_loss: 0.5286 - val_accuracy: 0.8763\n",
      "Epoch 201/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0965 - accuracy: 0.9631 - val_loss: 0.5407 - val_accuracy: 0.8678\n",
      "Epoch 202/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0912 - accuracy: 0.9641 - val_loss: 0.5341 - val_accuracy: 0.8743\n",
      "Epoch 203/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0897 - accuracy: 0.9655 - val_loss: 0.5059 - val_accuracy: 0.8727\n",
      "Epoch 204/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0938 - accuracy: 0.9636 - val_loss: 0.5197 - val_accuracy: 0.8643\n",
      "Epoch 205/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0965 - accuracy: 0.9631 - val_loss: 0.5024 - val_accuracy: 0.8750\n",
      "Epoch 206/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0865 - accuracy: 0.9674 - val_loss: 0.5536 - val_accuracy: 0.8708\n",
      "Epoch 207/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0892 - accuracy: 0.9658 - val_loss: 0.5179 - val_accuracy: 0.8763\n",
      "Epoch 208/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0860 - accuracy: 0.9667 - val_loss: 0.5267 - val_accuracy: 0.8692\n",
      "Epoch 209/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0851 - accuracy: 0.9671 - val_loss: 0.5248 - val_accuracy: 0.8755\n",
      "Epoch 210/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0945 - accuracy: 0.9629 - val_loss: 0.5420 - val_accuracy: 0.8730\n",
      "Epoch 211/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0884 - accuracy: 0.9658 - val_loss: 0.5758 - val_accuracy: 0.8652\n",
      "Epoch 212/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0872 - accuracy: 0.9669 - val_loss: 0.5451 - val_accuracy: 0.8690\n",
      "Epoch 213/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0923 - accuracy: 0.9649 - val_loss: 0.4945 - val_accuracy: 0.8762\n",
      "Epoch 214/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0835 - accuracy: 0.9685 - val_loss: 0.5277 - val_accuracy: 0.8770\n",
      "Epoch 215/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0769 - accuracy: 0.9705 - val_loss: 0.5394 - val_accuracy: 0.8715\n",
      "Epoch 216/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0815 - accuracy: 0.9692 - val_loss: 0.5278 - val_accuracy: 0.8757\n",
      "Epoch 217/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0847 - accuracy: 0.9675 - val_loss: 0.5651 - val_accuracy: 0.8640\n",
      "Epoch 218/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0893 - accuracy: 0.9664 - val_loss: 0.5177 - val_accuracy: 0.8782\n",
      "Epoch 219/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0825 - accuracy: 0.9683 - val_loss: 0.5535 - val_accuracy: 0.8708\n",
      "Epoch 220/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0867 - accuracy: 0.9667 - val_loss: 0.5407 - val_accuracy: 0.8737\n",
      "Epoch 221/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0819 - accuracy: 0.9685 - val_loss: 0.5350 - val_accuracy: 0.8778\n",
      "Epoch 222/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0844 - accuracy: 0.9672 - val_loss: 0.5334 - val_accuracy: 0.8753\n",
      "Epoch 223/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0841 - accuracy: 0.9673 - val_loss: 0.5356 - val_accuracy: 0.8708\n",
      "Epoch 224/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0839 - accuracy: 0.9684 - val_loss: 0.5238 - val_accuracy: 0.8755\n",
      "Epoch 225/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0925 - accuracy: 0.9644 - val_loss: 0.4989 - val_accuracy: 0.8760\n",
      "Epoch 226/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0820 - accuracy: 0.9686 - val_loss: 0.5358 - val_accuracy: 0.8732\n",
      "Epoch 227/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0811 - accuracy: 0.9684 - val_loss: 0.5413 - val_accuracy: 0.8653\n",
      "Epoch 228/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 0.9695 - val_loss: 0.5170 - val_accuracy: 0.8810\n",
      "Epoch 229/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0833 - accuracy: 0.9670 - val_loss: 0.5321 - val_accuracy: 0.8708\n",
      "Epoch 230/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0809 - accuracy: 0.9693 - val_loss: 0.5307 - val_accuracy: 0.8742\n",
      "Epoch 231/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0834 - accuracy: 0.9680 - val_loss: 0.5169 - val_accuracy: 0.8732\n",
      "Epoch 232/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0864 - accuracy: 0.9659 - val_loss: 0.5328 - val_accuracy: 0.8718\n",
      "Epoch 233/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0794 - accuracy: 0.9695 - val_loss: 0.5423 - val_accuracy: 0.8732\n",
      "Epoch 234/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0814 - accuracy: 0.9685 - val_loss: 0.5340 - val_accuracy: 0.8773\n",
      "Epoch 235/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0816 - accuracy: 0.9693 - val_loss: 0.5623 - val_accuracy: 0.8693\n",
      "Epoch 236/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1019 - accuracy: 0.9615 - val_loss: 0.5179 - val_accuracy: 0.8775\n",
      "Epoch 237/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0785 - accuracy: 0.9700 - val_loss: 0.5332 - val_accuracy: 0.8775\n",
      "Epoch 238/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0843 - accuracy: 0.9680 - val_loss: 0.5565 - val_accuracy: 0.8738\n",
      "Epoch 239/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0847 - accuracy: 0.9682 - val_loss: 0.5357 - val_accuracy: 0.8762\n",
      "Epoch 240/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0768 - accuracy: 0.9704 - val_loss: 0.5281 - val_accuracy: 0.8790\n",
      "Epoch 241/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0779 - accuracy: 0.9707 - val_loss: 0.5374 - val_accuracy: 0.8763\n",
      "Epoch 242/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0825 - accuracy: 0.9689 - val_loss: 0.5640 - val_accuracy: 0.8710\n",
      "Epoch 243/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0856 - accuracy: 0.9666 - val_loss: 0.5925 - val_accuracy: 0.8573\n",
      "Epoch 244/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0877 - accuracy: 0.9669 - val_loss: 0.5302 - val_accuracy: 0.8715\n",
      "Epoch 245/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0819 - accuracy: 0.9689 - val_loss: 0.5628 - val_accuracy: 0.8648\n",
      "Epoch 246/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0734 - accuracy: 0.9721 - val_loss: 0.5449 - val_accuracy: 0.8715\n",
      "Epoch 247/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0891 - accuracy: 0.9666 - val_loss: 0.5286 - val_accuracy: 0.8773\n",
      "Epoch 248/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0766 - accuracy: 0.9705 - val_loss: 0.5417 - val_accuracy: 0.8712\n",
      "Epoch 249/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0831 - accuracy: 0.9672 - val_loss: 0.5705 - val_accuracy: 0.8678\n",
      "Epoch 250/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0931 - accuracy: 0.9643 - val_loss: 0.5514 - val_accuracy: 0.8620\n",
      "Epoch 251/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0795 - accuracy: 0.9687 - val_loss: 0.5482 - val_accuracy: 0.8750\n",
      "Epoch 252/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9719 - val_loss: 0.5419 - val_accuracy: 0.8740\n",
      "Epoch 253/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0742 - accuracy: 0.9713 - val_loss: 0.5571 - val_accuracy: 0.8752\n",
      "Epoch 254/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0782 - accuracy: 0.9695 - val_loss: 0.5430 - val_accuracy: 0.8727\n",
      "Epoch 255/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0801 - accuracy: 0.9688 - val_loss: 0.5360 - val_accuracy: 0.8702\n",
      "Epoch 256/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0716 - accuracy: 0.9728 - val_loss: 0.5564 - val_accuracy: 0.8660\n",
      "Epoch 257/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0801 - accuracy: 0.9696 - val_loss: 0.5265 - val_accuracy: 0.8730\n",
      "Epoch 258/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0771 - accuracy: 0.9706 - val_loss: 0.5472 - val_accuracy: 0.8717\n",
      "Epoch 259/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0825 - accuracy: 0.9688 - val_loss: 0.6140 - val_accuracy: 0.8432\n",
      "Epoch 260/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0875 - accuracy: 0.9659 - val_loss: 0.5557 - val_accuracy: 0.8697\n",
      "Epoch 261/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0765 - accuracy: 0.9701 - val_loss: 0.6005 - val_accuracy: 0.8630\n",
      "Epoch 262/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9713 - val_loss: 0.5489 - val_accuracy: 0.8738\n",
      "Epoch 263/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0720 - accuracy: 0.9721 - val_loss: 0.5712 - val_accuracy: 0.8685\n",
      "Epoch 264/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0799 - accuracy: 0.9696 - val_loss: 0.5343 - val_accuracy: 0.8758\n",
      "Epoch 265/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0720 - accuracy: 0.9732 - val_loss: 0.5384 - val_accuracy: 0.8748\n",
      "Epoch 266/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0803 - accuracy: 0.9694 - val_loss: 0.5240 - val_accuracy: 0.8768\n",
      "Epoch 267/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0800 - accuracy: 0.9694 - val_loss: 0.5322 - val_accuracy: 0.8758\n",
      "Epoch 268/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0799 - accuracy: 0.9690 - val_loss: 0.5469 - val_accuracy: 0.8727\n",
      "Epoch 269/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0756 - accuracy: 0.9716 - val_loss: 0.5342 - val_accuracy: 0.8757\n",
      "Epoch 270/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0724 - accuracy: 0.9728 - val_loss: 0.5450 - val_accuracy: 0.8740\n",
      "Epoch 271/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0813 - accuracy: 0.9687 - val_loss: 0.5062 - val_accuracy: 0.8767\n",
      "Epoch 272/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0757 - accuracy: 0.9709 - val_loss: 0.5539 - val_accuracy: 0.8672\n",
      "Epoch 273/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0648 - accuracy: 0.9749 - val_loss: 0.6086 - val_accuracy: 0.8652\n",
      "Epoch 274/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0759 - accuracy: 0.9705 - val_loss: 0.5751 - val_accuracy: 0.8678\n",
      "Epoch 275/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0834 - accuracy: 0.9676 - val_loss: 0.5672 - val_accuracy: 0.8615\n",
      "Epoch 276/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0742 - accuracy: 0.9712 - val_loss: 0.5461 - val_accuracy: 0.8725\n",
      "Epoch 277/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0727 - accuracy: 0.9724 - val_loss: 0.5638 - val_accuracy: 0.8750\n",
      "Epoch 278/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0765 - accuracy: 0.9713 - val_loss: 0.5687 - val_accuracy: 0.8705\n",
      "Epoch 279/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9706 - val_loss: 0.5581 - val_accuracy: 0.8790\n",
      "Epoch 280/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0774 - accuracy: 0.9699 - val_loss: 0.5470 - val_accuracy: 0.8765\n",
      "Epoch 281/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0727 - accuracy: 0.9726 - val_loss: 0.5514 - val_accuracy: 0.8728\n",
      "Epoch 282/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0738 - accuracy: 0.9717 - val_loss: 0.5860 - val_accuracy: 0.8727\n",
      "Epoch 283/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0768 - accuracy: 0.9706 - val_loss: 0.5543 - val_accuracy: 0.8753\n",
      "Epoch 284/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0682 - accuracy: 0.9740 - val_loss: 0.5435 - val_accuracy: 0.8783\n",
      "Epoch 285/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0709 - accuracy: 0.9733 - val_loss: 0.5727 - val_accuracy: 0.8757\n",
      "Epoch 286/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9711 - val_loss: 0.5261 - val_accuracy: 0.8807\n",
      "Epoch 287/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0696 - accuracy: 0.9741 - val_loss: 0.5346 - val_accuracy: 0.8743\n",
      "Epoch 288/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0655 - accuracy: 0.9757 - val_loss: 0.5553 - val_accuracy: 0.8717\n",
      "Epoch 289/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0684 - accuracy: 0.9751 - val_loss: 0.5553 - val_accuracy: 0.8772\n",
      "Epoch 290/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0785 - accuracy: 0.9704 - val_loss: 0.5789 - val_accuracy: 0.8645\n",
      "Epoch 291/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0687 - accuracy: 0.9737 - val_loss: 0.6094 - val_accuracy: 0.8688\n",
      "Epoch 292/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0715 - accuracy: 0.9731 - val_loss: 0.5851 - val_accuracy: 0.8762\n",
      "Epoch 293/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0664 - accuracy: 0.9741 - val_loss: 0.5649 - val_accuracy: 0.8767\n",
      "Epoch 294/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0713 - accuracy: 0.9724 - val_loss: 0.5956 - val_accuracy: 0.8708\n",
      "Epoch 295/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0695 - accuracy: 0.9739 - val_loss: 0.5773 - val_accuracy: 0.8740\n",
      "Epoch 296/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0706 - accuracy: 0.9731 - val_loss: 0.5902 - val_accuracy: 0.8745\n",
      "Epoch 297/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0720 - accuracy: 0.9724 - val_loss: 0.5940 - val_accuracy: 0.8673\n",
      "Epoch 298/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0737 - accuracy: 0.9717 - val_loss: 0.5925 - val_accuracy: 0.8715\n",
      "Epoch 299/300\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0679 - accuracy: 0.9741 - val_loss: 0.5827 - val_accuracy: 0.8723\n",
      "Epoch 300/300\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0680 - accuracy: 0.9740 - val_loss: 0.5703 - val_accuracy: 0.8722\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  803840    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  262400    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  645       \n",
      "=================================================================\n",
      "Total params: 1,099,781\n",
      "Trainable params: 1,099,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          verbose = 1,\n",
    "          validation_data = (x_val, y_val))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WTQ5zBPz7TS6",
    "outputId": "6a886faa-b5de-4ade-a129-04f63ffb509c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40610218048095703\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "score = model.evaluate(x_val, y_val, verbose = 0)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "0J05QeZaUyDc",
    "outputId": "75e19ca6-5a9f-466e-b4f2-96c90935905d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.872167\n",
      "Precision: 0.872167\n",
      "Recall: 0.872167\n",
      "F1 score: 0.872167\n",
      "Cohens kappa: 0.840203\n",
      "ROC AUC: 0.982407\n",
      "[[[4724   86]\n",
      "  [  57 1133]]\n",
      "\n",
      " [[4604  166]\n",
      "  [ 172 1058]]\n",
      "\n",
      " [[4581  237]\n",
      "  [ 192  990]]\n",
      "\n",
      " [[4632  193]\n",
      "  [ 221  954]]\n",
      "\n",
      " [[4692   85]\n",
      "  [ 125 1098]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x_val, verbose=0)\n",
    "\n",
    "yhat_classes = yhat_probs.argmax(axis=-1)\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_val.argmax(axis=-1), yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_val.argmax(axis=-1), yhat_classes, average = 'micro')\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_val.argmax(axis=-1), yhat_classes, average = 'micro')\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_val.argmax(axis=-1), yhat_classes, average = 'micro')\n",
    "print('F1 score: %f' % f1)\n",
    " \n",
    "# kappa\n",
    "kappa = cohen_kappa_score(y_val.argmax(axis=-1), yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(y_val, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)\n",
    "\n",
    "matrix = multilabel_confusion_matrix(y_val.argmax(axis=-1), yhat_classes, labels = [0,1,2,3,4])\n",
    "print(matrix)\n",
    "\n",
    "#Reference:\n",
    "#https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z9j3XGEmPp_p",
    "outputId": "5ded06e3-6477-4968-b8cd-c532da97277e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 1, ..., 3, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test.drop(['Id'], axis = 1).to_numpy()\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_sol = model.predict(x_test)\n",
    "y_sol = y_sol.argmax(axis=-1)\n",
    "y_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPgbP2uCIhOc"
   },
   "outputs": [],
   "source": [
    "df_solution = pd.DataFrame(np.arange(0, x_test.shape[0]), columns=['Id'])\n",
    "df_solution['Label'] = y_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "E9dHWSjNY9GK",
    "outputId": "ba0e5eca-72bb-4755-8697-5814be382225"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Label\n",
       "0        0      3\n",
       "1        1      4\n",
       "2        2      1\n",
       "3        3      1\n",
       "4        4      1\n",
       "...    ...    ...\n",
       "9995  9995      1\n",
       "9996  9996      1\n",
       "9997  9997      3\n",
       "9998  9998      2\n",
       "9999  9999      2\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_solution"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DenseNet Assignment-3 Part-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
